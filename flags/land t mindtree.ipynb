{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d82be70",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk.translate' has no attribute 'moses'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m translator\u001b[38;5;241m=\u001b[39m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmoses\u001b[49m\u001b[38;5;241m.\u001b[39mMosesDetokenizer()\n\u001b[0;32m      3\u001b[0m translator\u001b[38;5;241m.\u001b[39mdetokenisze(nltk\u001b[38;5;241m.\u001b[39mtranslate\u001b[38;5;241m.\u001b[39mmoses\u001b[38;5;241m.\u001b[39mMosesTokenizer()\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello ,How are you?\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk.translate' has no attribute 'moses'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "translator=nltk.translate.moses.MosesDetokenizer()\n",
    "translator.detokenisze(nltk.translate.moses.MosesTokenizer().tokenize('Hello ,How are you?'),'fr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e031aa0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(...)? (1585562613.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    print words[i+1]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(...)?\n"
     ]
    }
   ],
   "source": [
    "s = \"this is a sentense\"\n",
    "target = \"is\"\n",
    "words = s.split()\n",
    "for i,w in enumerate(words):\n",
    "    if w == target:\n",
    "        # next word\n",
    "        print words[i+1]\n",
    "        # previous word\n",
    "        if i>0:\n",
    "            print words[i-1]s = \"this is a sentense\"\n",
    "target = \"is\"\n",
    "words = s.split()\n",
    "for i,w in enumerate(words):\n",
    "    if w == target:\n",
    "        # next word\n",
    "        print words[i+1]\n",
    "        # previous word\n",
    "        if i>0:\n",
    "            print words[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b231f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"This is a sentence\"\n",
    ">>> sl = s.split()\n",
    ">>> \n",
    ">>> def nextword(target, source):\n",
    "...   for i, w in enumerate(source):\n",
    "...     if w == target:\n",
    "...       return source[i+1]\n",
    "... \n",
    ">>> nextword('is', sl)\n",
    "'a'\n",
    ">>> nextword('a', sl)\n",
    "'sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a818ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def grade_essay(essay):\n",
    "    # tokenize the essay into words\n",
    "    words = word_tokenize(essay)\n",
    "\n",
    "    # get the total number of words in the essay\n",
    "    word_count = len(words)\n",
    "\n",
    "    # initialize the score\n",
    "    score = 0\n",
    "\n",
    "    # check for spelling errors\n",
    "    for word in words:\n",
    "        if not wordnet.synsets(word):\n",
    "            score -= 1\n",
    "\n",
    "    # check for grammar errors\n",
    "    text = nltk.Text(words)\n",
    "    grammar_check = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    for word in grammar_check:\n",
    "        if not wordnet.synsets(word):\n",
    "            score -= 1\n",
    "\n",
    "    # calculate the final score\n",
    "    final_score = score + (word_count / 100)\n",
    "\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4c98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def grade_essay(essay):\n",
    "    # tokenize the essay into words\n",
    "    words = word_tokenize(essay)\n",
    "\n",
    "    # get the total number of words in the essay\n",
    "    word_count = len(words)\n",
    "\n",
    "    # initialize the score\n",
    "    score = 0\n",
    "\n",
    "    # check for spelling errors\n",
    "    for word in words:\n",
    "        if not wordnet.synsets(word):\n",
    "            score -= 1\n",
    "\n",
    "    # check for grammar errors\n",
    "    text = nltk.Text(words)\n",
    "    grammar_check = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    for word in grammar_check:\n",
    "        if not wordnet.synsets(word):\n",
    "            score -= 1\n",
    "\n",
    "    # calculate the final score\n",
    "    final_score = score + (word_count / 100)\n",
    "\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8001ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\MY PC/nltk_data'\n    - 'C:\\\\Users\\\\MY PC\\\\New folder\\\\nltk_data'\n    - 'C:\\\\Users\\\\MY PC\\\\New folder\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\MY PC\\\\New folder\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\MY PC\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary\n\u001b[0;32m     30\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 32\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36msummarize\u001b[1;34m(text, num_sentences)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize\u001b[39m(text, num_sentences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Tokenize the text into sentences and words\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Remove stop words and punctuation\u001b[39;00m\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\MY PC/nltk_data'\n    - 'C:\\\\Users\\\\MY PC\\\\New folder\\\\nltk_data'\n    - 'C:\\\\Users\\\\MY PC\\\\New folder\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\MY PC\\\\New folder\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\MY PC\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def summarize(text, num_sentences=3):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    # Compute word frequencies\n",
    "    word_freq = nltk.FreqDist(words)\n",
    "\n",
    "    # Compute sentence scores based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_words = word_tokenize(sentence.lower())\n",
    "        sentence_score = sum([word_freq[word] for word in sentence_words if word in word_freq])\n",
    "        sentence_scores[i] = sentence_score\n",
    "\n",
    "    # Sort sentences by score and select top sentences\n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "\n",
    "    # Return the summary\n",
    "    summary = ' '.join([sentences[i] for i in sorted(top_sentences)])\n",
    "    return summary\n",
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "\n",
    "summary = summarize(text, num_sentences=2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1897e000",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3014948319.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71978788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] India is the seventh-largest country by area, and is bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[j] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.[29] Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE\n"
     ]
    }
   ],
   "source": [
    "def summarize(text, num_sentences=3):\n",
    "    # Split text into sentences and words\n",
    "    sentences = text.split(\".\")\n",
    "    words = [sentence.split() for sentence in sentences]\n",
    "\n",
    "    # Compute word frequencies\n",
    "    word_freq = {}\n",
    "    for sentence in words:\n",
    "        for word in sentence:\n",
    "            if word not in word_freq:\n",
    "                word_freq[word] = 1\n",
    "            else:\n",
    "                word_freq[word] += 1\n",
    "\n",
    "    # Compute sentence scores based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_score = sum([word_freq[word] for word in words[i]])\n",
    "        sentence_scores[i] = sentence_score\n",
    "\n",
    "    # Sort sentences by score and select top sentences\n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "\n",
    "    # Return the summary\n",
    "    summary = '.'.join([sentences[i] for i in sorted(top_sentences)])\n",
    "    return summary\n",
    "text = \"India, officially the Republic of India (IAST: Bhārat Gaṇarājya),[25] is a country in South Asia. It has been the second most populous country since at least 1950, with a current population of over 1.4 billion, and will overtake China to become the most populous country in 2023.[17] India is the seventh-largest country by area, and is bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[j] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.[26][27][28] Their long occupation, initially in varying forms of isolation as hunter-gatherers, has made the region highly diverse, second only to Africa in human genetic diversity.[29] Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE.[30] By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest.[31][32] Its evidence today is found in the hymns of the Rigveda. Preserved by a resolutely vigilant oral tradition, the Rigveda records the dawning of Hinduism in India.[33] The Dravidian languages of India were supplanted in the northern and western regions.[34] By 400 BCE, stratification and exclusion by caste had emerged within Hinduism,[35] and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity.[36] Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires based in the Ganges Basin.[37] Their collective era was suffused with wide-ranging creativity,[38] but also marked by the declining status of women,[39] and the incorporation of untouchability into an organised system of belief.[k][40] In South India, the Middle kingdoms exported Dravidian-languages scripts and religious cultures to the kingdoms of Southeast Asia.[41]\"\n",
    "summary = summarize(text, num_sentences=2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ac6d12",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2145102839.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 17\u001b[1;36m\u001b[0m\n\u001b[1;33m    img.save('grayscale.jpg'\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "img = Image.open('image.jpg')\n",
    "\n",
    "# Get the image dimensions\n",
    "width, height = img.size\n",
    "\n",
    "# Convert the image to grayscale\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        r, g, b = img.getpixel((x, y))\n",
    "        gray = int(0.2989 * r + 0.5870 * g + 0.1140 * b)\n",
    "        img.putpixel((x, y), (gray, gray, gray))\n",
    "\n",
    "# Save the new image\n",
    "img.save('grayscale.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce64c34f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Convert the bytes to an image\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(img_bytes))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get the image dimensions\u001b[39;00m\n\u001b[0;32m     16\u001b[0m width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Define the image URL\n",
    "url = 'https://i.imgur.com/ZZsB7VJ.jpg'\n",
    "\n",
    "# Download the image and convert it to bytes\n",
    "with urlopen(url) as response:\n",
    "    img_bytes = response.read()\n",
    "\n",
    "# Convert the bytes to an image\n",
    "img = image.open(io.BytesIO(img_bytes))\n",
    "\n",
    "# Get the image dimensions\n",
    "width, height = img.size\n",
    "\n",
    "# Convert the image to grayscale\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        r, g, b = img.getpixel((x, y))\n",
    "        gray = int(0.2989 * r + 0.5870 * g + 0.1140 * b)\n",
    "        img.putpixel((x, y), (gray, gray, gray))\n",
    "\n",
    "# Convert the image to a base64 string\n",
    "buffer = io.BytesIO()\n",
    "img.save(buffer, format='JPEG')\n",
    "img_str = base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "# Display the image in HTML\n",
    "html = '<img src=\"data:image/jpeg;base64,{}\">'.format(img_str)\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae632164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
